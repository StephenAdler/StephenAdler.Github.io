---
layout: post
title: 2021-03-18
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


# Today's achievement

I finished Chapter 2 and 3 of Hadley Wickham's *Advanced R* 2e. The main takeaway is as follows: 
    - copy-on-modify behavior is prevalent in R
    - one should remember some examples about the memory management rules in R
    - for example, when using a list, different items pointing to the same vector will not add too much burden on the memory
    - vectors are also prevalent in R. 4 main types of vectors: logical, double, integer, character
    - data frames and tibbles are **lists**.
    - say something about the differences between matrix, data frame and tibble

I watched Ryan Tibrashini's lecture on Sparsity, which is lecture 17 in the playlist https://www.youtube.com/watch?v=Qa9fmocsEvE&list=PLjbUi5mgii6B7A0nM74zHTOVQtTC9DaCv&index=17&ab_channel=RyanT

The main content includes: refreshing knowledge on the simple linear model. derive in-sample and out-of-sample measure of MSE. why we can't use simple LS in the high-dimensinal setting. l0,l1,l2 penalty and how they translate into best subset regression(unsuccessfully), lasso and ridge regression. why does the lasso has the sparsity feature(check the figure).

I read Belloni et al.'s 2013 JEP paper, which is about the double selection methods. Two examples should be enough to remember. In the first one, consider a IV setting, where they are a lot of potential IVs avaiable. The main regression is simple without variable selection issue. Then using lasso to select IV and then perform 2SLS based on selected IV is consistent. The validity is given by illustrated by the following lines:

>the structure of the problem is such that model selection mistakes in which a valid instrument with a small but nonzero coeffificient is left out of the fifirst-stage will not substantially affect the second-stage estimator of α as long as other instruments with large coefficients are selected. In other words, the second-stage instrumental variable estimate is orthogonal or immune to variable selection errors where instru- ments with small, nonzero coeffificients are mistakenly excluded from estimation. 

The second example is Inference with Selection among Many Controls. In this example, we are interested in the causal effect of $D$ on $Y$, with the presence of many regressors $X$. The idea is to regress $Y$ on $X$ and save all the relevant variables, regression $D$ on $X$ and save all the relevant variables, and finally regress $Y$ on the union of these relevant variables. This essentially rules out the possibility of dropping $X$ that highly correlates with $D$ but has little predictive power on $Y$. The key insight here is 

> It is also noteworthy that the “double selection” procedure implicitly estimates the residuals $\epsilon_i$ and $\nu_i$ and then regresses the estimates of $\epsilon_i$ on the estimates of $\nu_i$ to construct an estimator of $\alpha$, thereby providing a selection analog of Robinson’s (1988) method for estimating the parameters of a partially linear model to the high-dimensional case.

I read Horowitz's book on semiparametric and nonparametric methods. In particular, I skimmed through the Robinson Partially Linear Model. The identification and estimation of it is intuitive, but can be very useful in understanding Chernozhukov's recent works on Double Machine Learning. The idea is that those two "prediction" steps are Neyman orthogonal, meaning that the small perturbation of one will not affect the other, as argued above. When this kind of statistical behavior shows up, we would have very nice convergence rates. A next step is to read the follow-up papers by Chernizhukov et al.

I also read the beginning sections of Chapter 4 in Horowitz. It's about identification of binary response model. The model is specified as follows: $Y = 1(Y^* > 0)$, and $Y^* = X'\beta + U$. The goal is to identify and make inference about $\beta$. One thing Horowtiz emphasize is the introduction of random coefficient --- this means that we don't want to work with the assumption that $X$ and $U$ are independent.

For identification, we must impose scale and location normalization. A stunning result is that $E(U | X = x) = 0$ will not be enough for identifying $\beta$. The idea of the proof is that we can construct some variables $V_X$ such that $Y^* = X'b + V_X$ produces the same observable distribution. This kind of construction is available by considering a shifted logistic distribution and then relocate the mass without influencing 1) the mean 2) the probability associated with alternative param value.

The identification could be established under the conditional median independece assumption. The important thing here is that $S_1(b) = \{x: x'\beta < 0 \leq x'b \}$ and $S_2(b) = \{x: x'\beta \geq 0 > x'b \}$ can help with identification if the probability of their union is larger than zero. Suppose it holds for $S_1$, then it means, for $x \in S_1(b)$, if the true param is $\beta$, then $P(Y=1 | X = x) < 0.5$, and if the true param is $b$, then $P(Y=1 | X = x) \geq 0.5$. Some more technical details, like continuity of one regressor $x_1$, and that the regressors do not belong to a certain subspace, are required.

A next step is to relax those assumptions. I skipped those parts.

Next, consider the maximum score estimator, which is essentially a binary version of the LAD estimator. $median(Y \mid X = x) = \inf \{y: P(Y=y\mid X=x) \geq \frac{1}{2} \} = I(x'beta \geq 0)$. So we want to minimize $\sum \abs{Y_i - I(X_i^' \beta > 0)}$, and some transformation of it gives the familiar maximum score estimator.

